Reading is for losers.

Monte Carlo is our best friend.

"to combine the sampling of a continuous probability distribution (using ‘MCMC’) with the idea of Bayesian evidence to numerically determine which model is preferred as an explanation for a dataset"
Sections 4.1 and 5.1 of Statistical Techniques in Cosmology by Prof. Alan Heavens
https://arxiv.org/pdf/0906.0664v3.pdf

1. Data Generator
2. Model Selector
3. Test Model Selector against real data
4. Take over the world
5. Space colonisation

Timeline:
20161018
Meeting with Prof. Heavens

20161019
Worked on data generator
Able to generate arrays of points sampled from a Gaussian distribution, created histograms and scatterplots from data generated. Histograms can work for N dimensions, but a single scatterplot can visualise maximum 3 dimensions. 
Only using the graphs to show that data generator works. 
Began thinking about nearest neighbour problem.
Nearest neighbours: https://www.dataquest.io/blog/k-nearest-neighbors-in-python/
Nearest neighbours (but with a part regarding self learning): https://blog.cambridgecoding.com/2016/01/16/machine-learning-under-the-hood-writing-your-own-k-nearest-neighbour-algorithm/
Lambda functions: http://stackoverflow.com/questions/890128/why-are-python-lambdas-useful
Made nearest neighbour program
Research nearest neighbour constant of proportionality with number density?

Quotes:
20161019 
"We don't care about colour"
"We can't work without histograms"
